/** note: this only appears to work if you start your netcat listener prior to launching, eg:
nc -lk 9999.  But, to make things more useful, do something like this:
1) mkfifo /mapr/cluster/input
2) tail -f /mapr/cluster/input | nc -lk 9999
3) start this spark app to connect to the port
4) echo/cat stuff into /mapr/cluster/input, eg: `for i in SensorDataV5.csv |sort -k2 -k3 -t $',' `;do echo $i > in;sleep .01;done;
This should allow the connection to stay open, and be able to shove whatever you want into the FIFO and have it show up on the spark side.
*/

/*the idea here is that you'll have a netcat stream of CSV data coming in, and we need to:
1. insert/append the row into M7-tables
2. write/append the CSV to disk, for now just using loopback NFS.
*/

/*TODO:

- When saving RDD to disk, may be useful to output with the 'dateTime' squashed field instead of the date,time fields (for tableau)
- use KAFKA or another messaging system as the input (per Ted)
*/




package org.apache.spark.streaming.m7import

import java.io._
import scala.io.Source
import scala.util.Random
import org.apache.spark._
import org.apache.spark.rdd.RDD
import org.apache.spark.SparkContext._
import org.apache.spark.streaming._
import org.apache.spark.streaming.StreamingContext._
import org.apache.spark.streaming.{Seconds, StreamingContext}
import org.apache.spark.storage.StorageLevel
import org.apache.spark.util.IntParam
import org.apache.hadoop.hbase.HBaseConfiguration
import org.apache.hadoop.hbase.client.{HBaseAdmin,HTable,Put,Get}
import org.apache.hadoop.hbase.util.Bytes
import com.google.common.io.Files
import java.nio.charset.Charset
//for json conversion:
import org.json4s._
import org.json4s.JsonDSL._
import org.json4s.native.JsonMethods._

/** probably better to package the logging function up as a separate class, but for now this is fine */

import org.apache.spark.Logging
import org.apache.log4j.{Level, Logger}




object StreamingExamples extends Logging {

  /** Set reasonable logging levels for streaming if the user has not configured log4j. */
  def setStreamingLogLevels() {
    val log4jInitialized = Logger.getRootLogger.getAllAppenders.hasMoreElements
    if (!log4jInitialized) {
      // We first log something to initialize Spark's default logging, then we override the
      // logging level.
      logInfo("Setting log level to [WARN] for streaming example." +
        " To override add a custom log4j.properties to the classpath.")
      Logger.getRootLogger.setLevel(Level.WARN)
    }
    Logger.getRootLogger.setLevel(Level.WARN)
  }
}


/** now to the actual work..borrowed from spark streaming examples */

object m7import {
  def main(args: Array[String]) {
    if (args.length < 8) {
      System.err.println("Usage: m7import <master> <hostname> <port> <USERNAME> <batchsecs> </path/to/tablename> </path/to/outputFile> </path/to/d3.json>\n" +
        "In local mode, <master> should be 'local[n]' with n > 1")
      System.exit(1)
    }

  
    StreamingExamples.setStreamingLogLevels()

     val Array(master, host, IntParam(port), username, IntParam(batchsecs), tablename, outputPath, d3Input) = args
     val appName = "M7import" + "_" + username

     //time to write an output file..we should probably split it into multiples but for now we'll just stream to a single file.

    val outputFile = new File(outputPath)
    if (outputFile.exists()) {
      outputFile.delete()
      } 

    //force spark to a specific port to use for this context
    System.setProperty("spark.ui.port", "5050")
    System.setProperty("spark.cores.max", "2")

    // Create the context with a X second batch size, where X is the arg you supplied as 'batchsecs'.

    val ssc = new StreamingContext(master, appName, Seconds(batchsecs),
      System.getenv("SPARK_HOME"), StreamingContext.jarOfClass(this.getClass))

    //instantiate m7/hbase connection ahead of time.

    val conf = HBaseConfiguration.create()
    
    val admin = new HBaseAdmin(conf)

    //This assumes you already have a table created.  In theory you could have it create a table, but we won't get into that now.
    if(!admin.isTableAvailable(tablename )) {
        println("Table doesn't exist..quitting")
        System.exit(1)
        //admin.createTable(tableDesc)
      }

      val table = new HTable(conf, tablename)


    

    // Create a NetworkInputDStream on target ip:port and count the
    // words in input stream of \n delimited text (eg. generated by 'nc')
    val records = ssc.socketTextStream(host, port.toInt, StorageLevel.MEMORY_ONLY_SER)



    /*basically, foreach rdd inside the Dstream, perform a 'collect' on the RDD, which creates an array, 
    and run a foreach on the elements within the array.  Maybe there's a more 'sparky' way of doing this..
    */
    records.foreach(rdd => {
      val rddarray = rdd.collect
        if(rddarray.length > 0) {
          var linecount = 0
          for(line <- rddarray) {
            linecount += 1
             //split this row into words, from scala-cookbook, the .trim removes leading/trailing spaces from the values.
          
            val Array(resID, date, time, hz, disp, flo, sedPPM, psi, chlPPM) = line.split(",").map(_.trim)
            //since tableau is lame about datefields, need to combine date+time
            val dateTime = date + " " + time
            // Create a compositekey for M7 insertion
            val compositeKey = resID + "_" + dateTime
   
            //OLD: generate a random number to use for a key
            //val myKey=Random.nextInt(Integer.MAX_VALUE)

            val tblPut = new Put(Bytes.toBytes(compositeKey))
            //build our tblPut object with multiple columns.
            // TODO: probably better done w/ a loop, but that's for another day.

            tblPut.add(Bytes.toBytes("cf1"),Bytes.toBytes("resID"),Bytes.toBytes(resID))
            tblPut.add(Bytes.toBytes("cf1"),Bytes.toBytes("date"),Bytes.toBytes(dateTime))
            tblPut.add(Bytes.toBytes("cf1"),Bytes.toBytes("hz"),Bytes.toBytes(hz))
            tblPut.add(Bytes.toBytes("cf1"),Bytes.toBytes("disp"),Bytes.toBytes(disp))
            tblPut.add(Bytes.toBytes("cf1"),Bytes.toBytes("flo"),Bytes.toBytes(flo))
            tblPut.add(Bytes.toBytes("cf1"),Bytes.toBytes("sedPPM"),Bytes.toBytes(sedPPM))
            tblPut.add(Bytes.toBytes("cf1"),Bytes.toBytes("psi"),Bytes.toBytes(psi))
            tblPut.add(Bytes.toBytes("cf1"),Bytes.toBytes("chlPPM"),Bytes.toBytes(chlPPM))

            //finally, put the row
            table.put(tblPut)
            //append the row to a CSV file
            Files.append(resID + "," + dateTime + "," + hz + "," + disp + "," + flo + "," + sedPPM + "," + psi + "," + chlPPM + "\n", outputFile, Charset.defaultCharset())


          
          }
          /*Below example dumps the entire RDD to disk.
          note: this is sort of annoying..it saves to a new folder with a 'part-0000' file..just like a MR job
          */
          
          /*val csvDir = Random.nextInt(Integer.MAX_VALUE)
          rdd.saveAsTextFile("/mapr/shark/CSV/" + csvDir )
          */
          println("dumped " + linecount + " rows to table " + tablename + " and wrote them to " + outputPath)

        }
    })

/* This is how you save Dstreams out to a file, (one file per Dstream!) note though that it can even save 
EMPTY dstreams!  If you want to use it its best to insert some sort of check:
    records.saveAsTextFiles("/mapr/shark/CSV/")
*/
       //sliding window  this makes a new dstream containing the last 60 seconds of data..every 15 seconds
    val slidingWindow = records.window(Seconds(60), Seconds(15))

   /*
    Note that this only works because there's only one RDD per dstream in this case.
    Caveat: this results in the file being removed prior to inserting data into it..which can result in the file being 'gone' for a short period
      */
      val d3File = new File(d3Input)
      if (d3File.exists()) {
        d3File.delete()
      }
         /* basically, foreach rdd inside the Dstream, perform a 'collect' on the RDD, which creates an array, 
    and run a foreach on the elements within the array. There's probably a more elegant way of doing this.
        */
      val rddarray = rdd.collect
        if(rddarray.length > 0) {
          for(line <- rddarray) {
             /*time to split this row into words, from scala-cookbook, the .trim removes leading/trailing
             spaces from the values.
            */
            val Array(resID, date, time, hz, disp, flo, sedPPM, psi, chlPPM) = line.split(",").map(_.trim)
            //since tableau is lame about datefields, need to combine date+time
            val dateTime = date + " " + time
            //build a json blob
              val json = 
              ("PumpID" -> resID) ~
              ("date" -> date) ~
              ("time" -> time) ~
              ("HZ" -> hz) ~
              ("Displacement" -> disp) ~
              ("Flow" -> flo) ~
              ("SedimentPPM" -> sedPPM) ~
              ("PSI" -> psi) ~
              ("ChlorinepPM" -> chlPPM)\
              //append current record's JSON blob to output file.
            Files.append(pretty(render(json)) + "\n", d3File, Charset.defaultCharset())


          
          }
        }
    })




    // print lines to console
    //records.print()
    ssc.start()             // Start the computation
    ssc.awaitTermination()  // Wait for the computation to

  }
}